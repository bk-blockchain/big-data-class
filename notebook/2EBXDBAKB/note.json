{
  "paragraphs": [
    {
      "text": "%sh\n",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:22:27 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[![buildstatus](https://travis-ci.org/holdenk/learning-spark-examples.svg?branch\u003dmaster)](https://travis-ci.org/holdenk/learning-spark-examples)\nExamples for Learning Spark\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nExamples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\nin the mini-complete-example directory.\n\n\nThese examples have been updated to run against Spark 1.3 so they may\nbe slightly different than the versions in your copy of \"Learning Spark\".\n\nRequirements\n\u003d\u003d\n* JDK 1.7 or higher\n* Scala 2.10.3\n- scala-lang.org\n* Spark 1.3\n* Protobuf compiler\n- On debian you can install with sudo apt-get install protobuf-compiler\n* R \u0026 the CRAN package Imap are required for the ChapterSixExample\n* The Python examples require urllib3\n\nPython examples\n\u003d\u003d\u003d\n\nFrom spark just run ./bin/pyspark ./src/python/[example]\n\nSpark Submit\n\u003d\u003d\u003d\n\nYou can also create an assembly jar with all of the dependencies for running either the java or scala\nversions of the code and run the job with the spark-submit script\n\n./sbt/sbt assembly OR mvn package\ncd $SPARK_HOME; ./bin/spark-submit   --class com.oreilly.learningsparkexamples.[lang].[example] ../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar\n\n[![Learning Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url\u003dhttp%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp\u0026cjsku\u003d0636920028512)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574662918655_-206616700",
      "id": "20191125-062158_1831826300",
      "dateCreated": "Nov 25, 2019 6:21:58 AM",
      "dateStarted": "Nov 25, 2019 6:22:16 AM",
      "dateFinished": "Nov 25, 2019 6:22:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Loading text files",
      "text": "%pyspark\ninput \u003d sc.textFile(\"file:///usr/zeppelin/notebook/dataset/README.md\")\ninput.take(1)",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:24:32 AM",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[\u0027[![buildstatus](https://travis-ci.org/holdenk/learning-spark-examples.svg?branch\u003dmaster)](https://travis-ci.org/holdenk/learning-spark-examples)\u0027]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559878034066_-611957372",
      "id": "20190519-043619_1978877403",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:24:22 AM",
      "dateFinished": "Nov 25, 2019 6:24:22 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncat /usr/zeppelin/notebook/dataset/chung.txt/part-00001",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:26:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "* The Python examples require urllib3\n\nPython examples\n\u003d\u003d\u003d\n\nFrom spark just run ./bin/pyspark ./src/python/[example]\n\nSpark Submit\n\u003d\u003d\u003d\n\nYou can also create an assembly jar with all of the dependencies for running either the java or scala\nversions of the code and run the job with the spark-submit script\n\n./sbt/sbt assembly OR mvn package\ncd $SPARK_HOME; ./bin/spark-submit   --class com.oreilly.learningsparkexamples.[lang].[example] ../learning-spark-examples/target/scala-2.10/learning-spark-examples-assembly-0.0.1.jar\n\n[![Learning Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url\u003dhttp%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp\u0026cjsku\u003d0636920028512)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574663127060_-1124922024",
      "id": "20191125-062527_1993331158",
      "dateCreated": "Nov 25, 2019 6:25:27 AM",
      "dateStarted": "Nov 25, 2019 6:26:51 AM",
      "dateFinished": "Nov 25, 2019 6:26:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "File saving",
      "text": "%pyspark\ninput.saveAsTextFile(\u0027file:///usr/zeppelin/notebook/dataset/chung.txt\u0027)",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:28:24 AM",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1326242400141326214.py\", line 360, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"/usr/spark-2.2.0/python/pyspark/rdd.py\", line 1553, in saveAsTextFile\n    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/spark-2.2.0/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o117.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/usr/zeppelin/notebook/dataset/chung.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1326242400141326214.py\", line 367, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1326242400141326214.py\", line 360, in \u003cmodule\u003e\n    exec(code, _zcUserQueryNameSpace)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"/usr/spark-2.2.0/python/pyspark/rdd.py\", line 1553, in saveAsTextFile\n    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/spark-2.2.0/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.4/dist-packages/py4j-0.10.6-py3.4.egg/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o117.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/usr/zeppelin/notebook/dataset/chung.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559878034067_-612342121",
      "id": "20190519-063339_613615193",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:28:13 AM",
      "dateFinished": "Nov 25, 2019 6:28:13 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncat /usr/zeppelin/notebook/dataset/testweet.json\n",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:29:05 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "{\"createdAt\":\"Nov 4, 2014 4:56:59 PM\",\"id\":529799371026485248,\"text\":\"Adventures With Coffee, Code, and Writing.\",\"source\":\"\\u003ca href\\u003d\\\"http://twitter.com\\\" rel\\u003d\\\"nofollow\\\"\\u003eTwitter Web Client\\u003c/a\\u003e\",\"isTruncated\":false,\"inReplyToStatusId\":-1,\"inReplyToUserId\":-1,\"isFavorited\":false,\"retweetCount\":0,\"isPossiblySensitive\":false,\"contributorsIDs\":[],\"userMentionEntities\":[],\"urlEntities\":[],\"hashtagEntities\":[],\"mediaEntities\":[],\"currentUserRetweetId\":-1,\"user\":{\"id\":15594928,\"name\":\"Holden Karau\",\"screenName\":\"holdenkarau\",\"location\":\"\",\"description\":\"\",\"descriptionURLEntities\":[],\"isContributorsEnabled\":false,\"profileImageUrl\":\"http://pbs.twimg.com/profile_images/3005696115/2036374bbadbed85249cdd50aac6e170_normal.jpeg\",\"profileImageUrlHttps\":\"https://pbs.twimg.com/profile_images/3005696115/2036374bbadbed85249cdd50aac6e170_normal.jpeg\",\"isProtected\":false,\"followersCount\":1231,\"profileBackgroundColor\":\"C0DEED\",\"profileTextColor\":\"333333\",\"profileLinkColor\":\"0084B4\",\"profileSidebarFillColor\":\"DDEEF6\",\"profileSidebarBorderColor\":\"FFFFFF\",\"profileUseBackgroundImage\":true,\"showAllInlineMedia\":false,\"friendsCount\":600,\"createdAt\":\"Aug 5, 2011 9:42:44 AM\",\"favouritesCount\":1095,\"utcOffset\":-3,\"profileBackgroundImageUrl\":\"\",\"profileBackgroundImageUrlHttps\":\"\",\"profileBannerImageUrl\":\"\",\"profileBackgroundTiled\":true,\"lang\":\"en\",\"statusesCount\":6234,\"isGeoEnabled\":true,\"isVerified\":false,\"translator\":false,\"listedCount\":0,\"isFollowRequestSent\":false}}\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574663327692_1816222806",
      "id": "20191125-062847_1231567947",
      "dateCreated": "Nov 25, 2019 6:28:47 AM",
      "dateStarted": "Nov 25, 2019 6:28:54 AM",
      "dateFinished": "Nov 25, 2019 6:28:54 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Loading JSON",
      "text": "%pyspark\nimport json\ninput \u003d sc.textFile(\"file:///usr/zeppelin/notebook/dataset/testweet.json\")\ndata \u003d input.map(lambda x: json.loads(x))",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:30:45 AM",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1559878034067_-612342121",
      "id": "20190519-045600_1979822191",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:30:45 AM",
      "dateFinished": "Nov 25, 2019 6:30:45 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndata.first()[\u0027source\u0027]",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:31:12 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u0027\u003ca href\u003d\"http://twitter.com\" rel\u003d\"nofollow\"\u003eTwitter Web Client\u003c/a\u003e\u0027\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574663437817_-233810755",
      "id": "20191125-063037_378210605",
      "dateCreated": "Nov 25, 2019 6:30:37 AM",
      "dateStarted": "Nov 25, 2019 6:31:12 AM",
      "dateFinished": "Nov 25, 2019 6:31:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncat /usr/zeppelin/notebook/dataset/favourite_animals.csv\n",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:32:13 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "holden,panda\nnotholden,notpanda\nspark,bear"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574663517624_734540930",
      "id": "20191125-063157_1645655435",
      "dateCreated": "Nov 25, 2019 6:31:57 AM",
      "dateStarted": "Nov 25, 2019 6:32:03 AM",
      "dateFinished": "Nov 25, 2019 6:32:03 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Loading CVS and inspecting",
      "text": "%pyspark\ndf \u003d sqlContext.read.load(\u0027file:///usr/zeppelin/notebook/dataset/favourite_animals.csv\u0027, \n                          format\u003d\u0027com.databricks.spark.csv\u0027, \n                          header\u003d\u0027false\u0027, \n                          inferSchema\u003d\u0027true\u0027)",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:32:32 AM",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1559878034068_-614265866",
      "id": "20190519-060539_1637710538",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:32:16 AM",
      "dateFinished": "Nov 25, 2019 6:32:19 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ndf.printSchema()",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:34:44 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- _c0: string (nullable \u003d true)\n |-- _c1: string (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1574663542359_-486236682",
      "id": "20191125-063222_187349339",
      "dateCreated": "Nov 25, 2019 6:32:22 AM",
      "dateStarted": "Nov 25, 2019 6:34:32 AM",
      "dateFinished": "Nov 25, 2019 6:34:32 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Spark SQL",
      "text": "%pyspark\ntweets \u003d sqlContext.read.json(\"file:///usr/zeppelin/notebook/dataset/testweet.json\")\ntweets.registerTempTable(\"tweets\")\nresults \u003d sqlContext.sql(\"SELECT user.name, text FROM tweets Where user.name\u003d\u0027Holden Karau\u0027\")\nresults.show()",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:37:17 AM",
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------+--------------------+\n|        name|                text|\n+------------+--------------------+\n|Holden Karau|Adventures With C...|\n+------------+--------------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559878034069_-614650615",
      "id": "20190519-064143_1955968305",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:37:06 AM",
      "dateFinished": "Nov 25, 2019 6:37:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shell command",
      "text": "%sh\npwd\nls -la /usr/zeppelin/notebook/dataset",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:20:54 AM",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "/usr/zeppelin\ntotal 4288\ndrwxr-xr-x  9 root root     288 Jun  7 03:31 .\ndrwxr-xr-x 20 root root     640 Jun  7 03:30 ..\n-rw-r--r--  1 root root   95694 Jun  6 11:40 2008-small.csv\n-rw-r--r--  1 root root    7080 Jun  7 02:50 2015-summary.csv\n-rw-r--r--  1 root root 3974305 Jun  7 00:57 adult.data\n-rwxr-xr-x  1 root root      42 Jan 28  2016 favourite_animals.csv\n-rwxr-xr-x  1 root root    1622 Jan 28  2016 README.md\ndrwxr-xr-x  8 root root     256 Jun  7 03:30 save\n-rwxr-xr-x  1 root root    1493 Jan 28  2016 testweet.json\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559878034069_-614650615",
      "id": "20190519-044045_15350527",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Jun 7, 2019 3:31:40 AM",
      "dateFinished": "Jun 7, 2019 3:31:42 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\npwd\n",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:49:00 AM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "/usr/zeppelin\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1559878034069_-614650615",
      "id": "20190519-044439_2103385607",
      "dateCreated": "Jun 7, 2019 3:27:14 AM",
      "dateStarted": "Nov 25, 2019 6:48:48 AM",
      "dateFinished": "Nov 25, 2019 6:48:48 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n",
      "user": "anonymous",
      "dateUpdated": "Nov 25, 2019 6:48:48 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1574664528174_40305606",
      "id": "20191125-064848_387974070",
      "dateCreated": "Nov 25, 2019 6:48:48 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/Big-data-class/Module-08/Loading-data",
  "id": "2EBXDBAKB",
  "angularObjects": {
    "2ESP3P9MZ:shared_process": [],
    "2ETXU2AC5:shared_process": [],
    "2EUJFKGWF:shared_process": [],
    "2ET3YD23J:shared_process": [],
    "2EWA5BRM8:shared_process": [],
    "2ETAXZR8W:shared_process": [],
    "2EUDEW6HQ:shared_process": [],
    "2EVV1F1J7:shared_process": [],
    "2EVSJTT4W:shared_process": []
  },
  "config": {},
  "info": {}
}